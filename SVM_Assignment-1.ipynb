{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\t1.\tMathematical Formula for Linear SVM:\n",
    "\t•\tExplain the linear SVM function ￼, where ￼ is the weight vector and ￼ is the bias term. Discuss how this function separates data points into different classes.\n",
    "\t2.\tObjective Function of Linear SVM:\n",
    "\t•\tDescribe the optimization problem: minimize ￼ subject to the constraint ￼ for each data point ￼. Explain how this objective aims to maximize the margin between classes.\n",
    "\t3.\tKernel Trick in SVM:\n",
    "\t•\tDefine the kernel trick as a method for transforming data into a higher-dimensional space to make it separable. Provide a few examples of popular kernels (polynomial, Gaussian, RBF) and their usage.\n",
    "\t4.\tRole of Support Vectors in SVM:\n",
    "\t•\tExplain that support vectors are the data points closest to the decision boundary and directly affect the position of the hyperplane. Use an example with a simple dataset to show how these points impact the classifier.\n",
    "\t5.\tHyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM:\n",
    "\t•\tIllustrate each concept with diagrams:\n",
    "\t•\tHyperplane: A line or plane that separates classes.\n",
    "\t•\tMarginal Plane: Planes parallel to the hyperplane, at the distance of the margin.\n",
    "\t•\tHard Margin: Only works for linearly separable data.\n",
    "\t•\tSoft Margin: Allows for some misclassifications, balancing margin maximization and error minimization.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
